---
title: "Machine Learning with NHANES Data"
author:
  - name: Grace Ivins
    url: https://github.com/graceivins
date: "4/16/2020"
output: radix::radix_article
---

This project uses machine learning algorithms to predict systolic blood pressure in the NHANES data set. The National Health and Nutrition Examination Survey (NHANES) has been conducted since 1971 and aims to assess the mental and physical wellbeing of children and adults in the United States. Each participant completes a questionnaire, is physically examined by a medical health professional, and undergoes laboratory tests. 

I have several goals for this project. I am primarily interested in learning the basics of machine learning and how machine learning differs from my master's discipline, statistics. Secondary goals include learning how to implement machine learning algorithms using packages in python and learning how to integrate R and Python to create reports that are useful and easy to read. 

High systolic blood pressure is an important indicator of health, particularly for those over the age of 50, even when diastolic blood pressure is normal. Machine learning algortihms will allow us to use the wealth of NHANES data to predict systolic blood pressure with good accuracy. Why would we want to predict something that can be easily measured at a doctor's office? There are several reasons we might want to predict items that are relatively easy to measure. For example, we might want to create an online tool where people could input factors they can easily measure themselves, like height, weight, and age, that then predicts their systolic blood pressure. We might also want to know what factors contribute significantly to the prediction so that health campaigns are targeted to the right groups.


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,      # Output code chunks
    message = FALSE,  # Toggle off message output 
    warning = FALSE)  # Toggle off warning output
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(reticulate)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
conda_list()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
use_condaenv("anaconda3")
```

```{python, echo=F, warning=F, message=F}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as seabornInstance
import scipy as sc
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics 
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
```

When undertaking a data project, be it in statistics or machine learning, thorough inspection and cleaning are essential. The first thing I noticed when looking at the NHANES data was that it has quite a few NaN's. There are several options for dealing with missing values, like imputation and so forth. We will deal with them by removing any observations (rows) that include NaN's. This will have implications as we are removing individuals from the data. However, most of the NaN's occur in the alcohol units per day column as children were also a part of the study. 

Since our concern here is systolic blood pressure, we aren't too worried about preserving the measurements on children.

```{python, message=F, warning=F, echo=F}
datanan = pd.read_csv("Nhanes_ML.csv", sep = ",")
data=datanan.dropna()
```

First, we have a look at the first few rows of the data.

```{python}
print(data.head())
```

This is a medium sized data set. We can see all variables more clearly if we "glimpse" the data instead using the tidyverse library in R:

```{r}
library(tidyverse)

py$data %>% 
    as.tibble() %>%
    glimpse()
```

Let's have a look at a correlation matrix:

```{r}
corr <- cor(py$data)
round(corr, 2)
```

We will exclude BMI as BMI is a ratio of height and weight.

```{python}
data=data.drop("BMI",axis=1)
```

Since income and poverty (defined as the ratio of family income to poverty guidelines) are highly correlated, we will also remove income as the information the variables contribute to the prediction will be redundant. If we wanted to make inference, as we often do in statistics, we would be concerned about multicollinearity. In ML, we are often primarily concerned with making predictions and so aren't as concerned with multicollinearity.

```{python}
data=data.drop("HHIncomeMid",axis=1)
```

Take a peek at our new correlation matrix:

```{r}
corr <- cor(py$data)
round(corr, 2)
```

What's going on with this data? Well, not much, and that can sometimes happen with data collected in the real world. We will continue to use this data set to learn more about ML. We need to do more exploratory data analysis before we can start running our model.  

Let's start with summary statistics. What we are looking for here are reasonable values. If one of our variables is Age, for example, we would be wary of a maximum of 110. The means and quantiles look reasonable except for our response variable, BPSysAve, and alcohol units per day. Notice that the third quartile (75th percentile) for the alcohol variable looks reaosnable at a value of 3 but that the maximum is 82 which is clearly an error. 

```{python}
data.describe()
data['BPSysAve'].describe()
```

Outliers can be an important consideration before applying many machine learning algorithms. Ideally, we would like all of our data points to make equivalent contributions to our predictions rather than allowing one or a handfull of observations to skew the results. We will deal with outliers before we conduct our analyses by standardizing our data and exluding values that are more than three standard deviations away from the mean.

```{python}
z_scores = sc.stats.zscore(data)
abs_z_scores = np.abs(z_scores)
filtered_entries = (abs_z_scores < 3).all(axis=1)
data = data[filtered_entries]
```

Let's look at those summary stats again to check that the values, particularly for alcohol units per day and systolic BP, look more reaosnable. Notice now that the maximum number of alcohol units per day is still quite high at 12 units, but not so high that no human being could possibly consume the amount in one day.

```{python}
data.describe()
data['BPSysAve'].describe()
```

Another important consideration is the overall distribution of the variables, particularly for the response variable, BPSysAve. The variables poverty and alcohol units per day are heavily skewed. It does look like average systolic blood pressure is approximately normal. In statistics, it is important that assumptions like normality of the response variable are met before conducting certain types of analyses like regression. In ML, we may not be as concerned with these assumptions.

```{r}
par(mfrow=c(3,3))
for (i in 1:ncol(py$data)){
  hist(py$data[,i],main=colnames(py$data[i]),xlab=colnames(py$data[i]))
}
```

Regression is one of two types of supervised machine learning. The other is classification (where the outcome is categorical or discrete-valued). Regression may be an appropriate model in this case as our outcome is continuous and quantitative. When we are finished with the analysis, we will have to assess the model's utility and determine whether the fit is good.

To get started, we will split the data into a test set and a training set. The test_size option indicates what proportion of the data will be used as the test set. The random_state option indicates what seed will be used. 

```{python}
y = data.BPSysAve
X = data.drop("BPSysAve", axis=1)
X_train, X_test, y_train, y_test = train_test_split(
  X, y,
  test_size    = 0.2,
  random_state = 100
)
```

Now, we need to transform the data. Regression ML algorithms can be sensitive to large differences in magnitudes across variables. We standardize the values, $x$ by subtracting the mean of that particular variable, $\mu$, and dividing by the standard deviation, $\sigma$.

$$z=(x-\mu)/\sigma$$

Technically, $\mu$ and $\sigma$ are quantities in the population that we can never truly know. We will use our training set to approximate these values and then apply the same transformation to our test set.

```{python}
scaler = preprocessing.StandardScaler().fit(X_train)
```

After standardizing, the mean should be near 0 and the standard deviaiton should be near 1. Let's check whether this is approximately true.

```{python}
X_test_scaled = scaler.transform(X_test)
X_test_scaled.mean(axis=0)
X_test_scaled.std(axis=0)
```

Now, we will begin the training phase for our first model choice, multiple linear regression. Using the code below, we are telling our machine to learn which coefficients for each term will allow us to make the best predictions for our outcome, average systolic blood pressure. In other words, the function LinearRegression() minimizes the residual sum of squares. If we implement regression in this way, we are doing exactly what statisticians do when they run a linear regression model. The only differences are in how we will use our model output and the fact that we split our data into a test set and training set. That being said, these practices are also fairly commonly done in statistics as well.

```{python}
regressor=LinearRegression()
regressor.fit(X_train,y_train)
```

Let's see what coefficients have been chosen:

```{python}
coeff_df = pd.DataFrame(regressor.coef_, X.columns, columns=['Coefficient'])  
coeff_df
```

We want to be careful about our language when we interpret coefficients. We say that for a one unit increase in a given variable the change in systolic blood pressure is given by the coefficient, while holding all other predictors constant. The last bit there is key and should not be discarded. For example, for eevery one unit increase in total cholesterol, systolic blood pressure tends to increase by 1.4 while holding all other predictors constant.

Using our test set, we will make predictions for our response variable using the coefficients above:

```{python}
y_pred = regressor.predict(X_test)
```

We want to see how well our model fits. Let's compute and examine our residuals. A residual is the difference between the observed value in a data set and a predicted value. Let's create a data frame that collects and stores our fitted and predicted values.

```{python}
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df.head(25)
df.describe()
```

Notice that the maximum predicted value is about 134 and the maximum observed value is 164. This suggests that the model may not be fitting well on the extremes. Let's use ggplot2 in R to plot the observed and predicted values against one another:

```{r}
library(tidyverse)
library(tidyquant) # for theme_tq()

# Manipulate data for ggplot
results_tbl <- tibble(
    y_test = py$y_test,
    y_pred = py$y_pred
) %>%
    rowid_to_column() %>%
    arrange(y_test) %>%
    mutate(rowid = as_factor(as.character(rowid))) %>%
    rowid_to_column("sorted_rowid") %>%
    gather(key = "key", value = "value", -c(rowid, sorted_rowid)) 

# Make ggplot
results_tbl %>%
    ggplot(aes(sorted_rowid, value, color = key)) +
    geom_point(alpha = 0.5) +
    geom_smooth() + 
    theme_tq() +
    scale_color_tq() +
    labs(
        title = "Prediction Versus Actual",
        subtitle = "Systolic BP",
        x = "Sorted RowID", y = "BPSysAve"
    )
```

You can fairly easily see in the plot that this is not at all a good fit, especially for values on the extreme ends. Let's have a look at the residuals:

```{r}
residuals=py$y_test-py$y_pred
mean(residuals)
plot(residuals)
abline(h=0)
```

Ideally, we would like the mean of the residuals to be near 0, and they are. Also the residual plot looks pretty good. So, it looks like our model isn't fitting the extremes well, but it is misfitting them consistently. This would be a problem if we want to be able to predict extreme values. Let's get a value of $R^2$ for the multiple linear regression model. $R^2$ gives the proportion of variability in the response that is explained by the predictor variables.

```{python}
print(r2_score(y_test, y_pred))
```

For this model, about 19% of the variability in the response is explained by the predictors. Let's try another model. Random forest regression is another popular machine learning algorithm that is easy to implement in python. 

Like linear regression, random forest regression is a supervised learning algorithm, meaning that inputs and outputs are clearly defined. Unlike linear regression, random forest regression is an ensemble learning algorithm. This means that predictions from many regression trees will be averaged. Regression trees discretize the values of the predictors and choose how to do that such that the residual sum of squares is minimized. This technique uses bootstrapping where random samples are taken with replacement and the trees run in parallel, independently of one another, until the results are averaged.

To set up random forest egression in python, we can scale and set up the model simultaneously using a pipeline. Here n_estimators gives the number of trees. Generally, the more trees you have the better your predictions will be, but there is a point where predictions get only marginally better with too great a cost in computation time. To choose 100 trees, I checked the fit for 10, 100, and 200 trees and determined that 100 was optimal based on time of computation and accuracy of predictions.

```{python}
pipeline = make_pipeline(
    preprocessing.StandardScaler(),
    RandomForestRegressor(n_estimators = 100)
    )
```

In the code below the hyperparameters, maximum features and maximum depth, are given multiple options from which to choose. Maximum features gives the number of features or variables to consider when looking for the best split. The option are "auto", max_features=number of predictor variables, "sqrt", max_features=square root of the number of variables in the data set, and "log2", max_features=log base 2 of the number of predictor variables in the data.

The hyperparameter max_depth gives the maximum depth of the tree. The depth of a decision tree is the length of the longest path from a root to a leaf. This can take integer values or "None". If "None" is chosen, then nodes are expanded until all leaves are pure, in other words when all of the data belongs to a single class, or discretized group in this case as all of our predictors are quantitative.

```{python}
hyperparameters = {
    "randomforestregressor__max_features" : ["auto", "sqrt", "log2"],
    "randomforestregressor__max_depth"    : [None, 5, 3, 1]
}
```

GridSearchCV exhaustively searchers for the optimal values of the hyperparameters by trying all combinations. When cv=None, the grid search is completed using 5-fold cross validation (CV). Cross validation is essentially a model validation technique that is used when prediction is the goal. In general, $k$-fold CV partitions the data into $k$ subsets. Below, we have set cv=10, so we are using 10-fold CV.

```{python}
clf = GridSearchCV(pipeline, hyperparameters, cv = 10)
clf.fit(X_train, y_train)
```

Using the command "best_params_" below we can see which values for the hyperparameters were chosen. Max depth was chosen to be "None" and max features was chosen to be "log2".

```{python}
print(clf.best_params_)
```

Now, we want to see how well the random forest model fit the data. First we will look at the value of $R^2$. The value is much higher than for our multiple linear regression model. Here, the proportion of variability that is explained by the predictors is about 56%. 

```{python}
y_pred = clf.predict(X_test)
print(r2_score(y_test, y_pred))
```

```{python}
print(mean_squared_error(y_test, y_pred))
```

Let's have a look at our observed and predicted values:

```{python}
df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
df.head(25)
df.describe()
```

We'll also plot our observed values against our predicted values:

```{r}
library(tidyverse)
library(tidyquant) # for theme_tq()

# Manipulate data for ggplot
results_tbl <- tibble(
    y_test = py$y_test,
    y_pred = py$y_pred
) %>%
    rowid_to_column() %>%
    arrange(y_test) %>%
    mutate(rowid = as_factor(as.character(rowid))) %>%
    rowid_to_column("sorted_rowid") %>%
    gather(key = "key", value = "value", -c(rowid, sorted_rowid)) 

# Make ggplot
results_tbl %>%
    ggplot(aes(sorted_rowid, value, color = key)) +
    geom_point(alpha = 0.5) +
    geom_smooth() + 
    theme_tq() +
    scale_color_tq() +
    labs(
        title = "Prediction Versus Actual",
        subtitle = "Systolic BP",
        x = "Sorted RowID", y = "BPSysAve"
    )
```

A look at the residuals:

```{r}
residuals=py$y_test-py$y_pred
mean(residuals)
plot(residuals)
abline(h=0)
```

